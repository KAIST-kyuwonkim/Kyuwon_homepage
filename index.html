<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <title>ê¹€ê·œì› | í¬íŠ¸í´ë¦¬ì˜¤</title>
  <link rel="stylesheet" href="css/style.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
  <header>
    <img src="assets/ì‚¬ì§„.png" alt="Profile" class="profile">
    <h1>Kyuwon Kim</h1>
    <p>ğŸ‘¨â€ğŸ“ M.S.â€“Ph.D. student â”‚ KAIST</p>
    <p>I am an integrated M.S.â€“Ph.D. student in the Optimization Laboratory at the Department of Mathematical Sciences, Korea Advanced Institute of Science and Technology (KAIST), where I am fortunate to be advised by 
  <a href="https://mathsci.kaist.ac.kr/~donghwankim/" target="_blank">Prof. Donghwan Kim</a>.
    </p>
  </header>
  <section id="work">
    <h2>ğŸ“š Research Interests</h2>
    <ul>
      <li><a href="#">Enhancing Generalization in Over-Parametrized Generative Models</a></li>
      <li><a href="#">Minimax Optimization</a></li>
    </ul>
  </section>
  <section id="publication">
    <h2>ğŸ§¾ Publication</h2>
    <ul>
      <li><a href="https://proceedings.mlr.press/v195/chae23a.html">Open Problem: Is There a First-Order Method that Only Converges to Local Minimax Optima?</a><\li>
      <li><a href="https://openreview.net/forum?id=6CIGhcJYJH">Two-timescale Extragradient for Finding Local Minimax Points</a><\li>
      <li><a href="https://proceedings.mlr.press/v235/kim24m.html">Double-Step Alternating Extragradient with Increasing Timescale Separation for Finding Local Minimax Points: Provable Improvements</a><\li>
      <li><a href="https://https://openreview.net/forum?id=JIMZsqE8bA">Rethinking Memorizationâ€“Generalization Trade-Off in Generative Models</a><\li>
    </ul>
  </section>
  <section id="contact">
    <h3>ğŸ“¬ Contact</h3>
    <p>
      <a href="https://scholar.google.com/citations?user=S8Hnn7wAAAAJ&hl=ko">Google Scholar</a> |
      <a href="kkw4053@kaist.ac.kr">Email</a>
    </p>
  </section>
</body>
</html>

<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <title>김규원 | 포트폴리오</title>
  <link rel="stylesheet" href="css/style.css">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
  <header>
    <img src="asset/profile.jpg" alt="Profile" class="profile">
    <h1>Kyuwon Kim</h1>
    <p>👨‍🎓 M.S.–Ph.D. student │ KAIST</p>
    <p>I am an integrated M.S.–Ph.D. student in the Optimization Laboratory at the Department of Mathematical Sciences, Korea Advanced Institute of Science and Technology (KAIST), where I am fortunate to be advised by 
  <a href="https://mathsci.kaist.ac.kr/~donghwankim/" target="_blank">Prof. Donghwan Kim</a>.
    </p>
  </header>
  <section id="work">
    <h2>📚 Research Interests</h2>
    <ul>
      <li><a href="#">Enhancing Generalization in Over-Parametrized Generative Models</a></li>
      <li><a href="#">Minimax Optimization</a></li>
    </ul>
  </section>
  <section id="publication">
    <h2>📄 Publications</h2>
    <ul>
      <li>
        <a href="https://proceedings.mlr.press/v195/chae23a.html">
          Open Problem: Is There a First-Order Method that Only Converges to Local Minimax Optima?
        </a><br>
        Jiseok Chae, <strong>Kyuwon Kim</strong>, Donghwan Kim<br>
        <em>Conference on Learning Theory (COLT), 2023.</em>
      </li>
      <li>
        <a href="https://openreview.net/forum?id=6CIGhcJYJH">
          Two-timescale Extragradient for Finding Local Minimax Points
        </a><br>
        Jiseok Chae, <strong>Kyuwon Kim</strong>, Donghwan Kim<br>
        <em>International Conference on Learning Representations (ICLR), 2024.</em>
      </li>
      <li>
        <a href="https://proceedings.mlr.press/v235/kim24m.html">
          Double-Step Alternating Extragradient with Increasing Timescale Separation for Finding Local Minimax Points: Provable Improvements
        </a><br>
        <strong>Kyuwon Kim</strong>, Donghwan Kim<br>
        <em>International Conference on Machine Learning (ICML), 2024.</em>
      </li>
      <li>
        <a href="https://openreview.net/forum?id=JIMZsqE8bA">
          Rethinking Memorization–Generalization Trade-Off in Generative Models
        </a><br>
        Jiseok Chae*, <strong>Kyuwon Kim*</strong>, Donghwan Kim<br>
        <em>High-dimensional Learning Dynamics (HiLD) Workshop at International Conference on Machine Learning (ICML), 2025.</em>
      </li>
    </ul>
  </section>
  <section id="contact">
    <h3>📬 Contact</h3>
    <p>
      <a href="https://scholar.google.com/citations?user=S8Hnn7wAAAAJ&hl=ko">Google Scholar</a> |
      <a href="kkw4053@kaist.ac.kr">Email</a>
    </p>
  </section>
</body>
</html>
